--------------------------------
official configurations
---------------------------
fluent.conf
---------------

# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/fluent.conf.erb

@include "#{ENV['FLUENTD_SYSTEMD_CONF'] || 'systemd'}.conf"
@include "#{ENV['FLUENTD_PROMETHEUS_CONF'] || 'prometheus'}.conf"
@include kubernetes.conf
@include conf.d/*.conf

<match **>
   @type elasticsearch
   @id out_es
   @log_level info
   include_tag_key true
   host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
   port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
   path "#{ENV['FLUENT_ELASTICSEARCH_PATH']}"
   scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
   ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
   ssl_version "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERSION'] || 'TLSv1'}"
   reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'false'}"
   reconnect_on_error "#{ENV['FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR'] || 'true'}"
   reload_on_failure "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE'] || 'true'}"
   log_es_400_reason "#{ENV['FLUENT_ELASTICSEARCH_LOG_ES_400_REASON'] || 'false'}"
   logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
   logstash_format "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT'] || 'true'}"
   index_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_INDEX_NAME'] || 'logstash'}"
   type_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_TYPE_NAME'] || 'fluentd'}"
   <buffer>
     flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
     flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
     chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
     queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
     retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
     retry_forever true
   </buffer>
</match>
-------------------------------------
kubernetes.conf
--------------------------------------

# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/kubernetes.conf.erb

<match fluent.**>
  @type null
</match>

<source>
  @type tail
  @id in_tail_container_logs
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type "#{ENV['FLUENT_CONTAINER_TAIL_PARSER_TYPE'] || 'json'}"
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

<source>
  @type tail
  @id in_tail_minion
  path /var/log/salt/minion
  pos_file /var/log/fluentd-salt.pos
  tag salt
  <parse>
    @type regexp
    expression /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
    time_format %Y-%m-%d %H:%M:%S
  </parse>
</source>

<source>
  @type tail
  @id in_tail_startupscript
  path /var/log/startupscript.log
  pos_file /var/log/fluentd-startupscript.log.pos
  tag startupscript
  <parse>
    @type syslog
  </parse>
</source>

<source>
  @type tail
  @id in_tail_docker
  path /var/log/docker.log
  pos_file /var/log/fluentd-docker.log.pos
  tag docker
  <parse>
    @type regexp
    expression /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
  </parse>
</source>

<source>
  @type tail
  @id in_tail_etcd
  path /var/log/etcd.log
  pos_file /var/log/fluentd-etcd.log.pos
  tag etcd
  <parse>
    @type none
  </parse>
</source>

<source>
  @type tail
  @id in_tail_kubelet
  multiline_flush_interval 5s
  path /var/log/kubelet.log
  pos_file /var/log/fluentd-kubelet.log.pos
  tag kubelet
  <parse>
    @type kubernetes
  </parse>
</source>

<source>
  @type tail
  @id in_tail_kube_proxy
  multiline_flush_interval 5s
  path /var/log/kube-proxy.log
  pos_file /var/log/fluentd-kube-proxy.log.pos
  tag kube-proxy
  <parse>
    @type kubernetes
  </parse>
</source>

<source>
  @type tail
  @id in_tail_kube_apiserver
  multiline_flush_interval 5s
  path /var/log/kube-apiserver.log
  pos_file /var/log/fluentd-kube-apiserver.log.pos
  tag kube-apiserver
  <parse>
    @type kubernetes
  </parse>
</source>

<source>
  @type tail
  @id in_tail_kube_controller_manager
  multiline_flush_interval 5s
  path /var/log/kube-controller-manager.log
  pos_file /var/log/fluentd-kube-controller-manager.log.pos
  tag kube-controller-manager
  <parse>
    @type kubernetes
  </parse>
</source>

<source>
  @type tail
  @id in_tail_kube_scheduler
  multiline_flush_interval 5s
  path /var/log/kube-scheduler.log
  pos_file /var/log/fluentd-kube-scheduler.log.pos
  tag kube-scheduler
  <parse>
    @type kubernetes
  </parse>
</source>

<source>
  @type tail
  @id in_tail_rescheduler
  multiline_flush_interval 5s
  path /var/log/rescheduler.log
  pos_file /var/log/fluentd-rescheduler.log.pos
  tag rescheduler
  <parse>
    @type kubernetes
  </parse>
</source>

<source>
  @type tail
  @id in_tail_glbc
  multiline_flush_interval 5s
  path /var/log/glbc.log
  pos_file /var/log/fluentd-glbc.log.pos
  tag glbc
  <parse>
    @type kubernetes
  </parse>
</source>

<source>
  @type tail
  @id in_tail_cluster_autoscaler
  multiline_flush_interval 5s
  path /var/log/cluster-autoscaler.log
  pos_file /var/log/fluentd-cluster-autoscaler.log.pos
  tag cluster-autoscaler
  <parse>
    @type kubernetes
  </parse>
</source>

# Example:
# 2017-02-09T00:15:57.992775796Z AUDIT: id="90c73c7c-97d6-4b65-9461-f94606ff825f" ip="104.132.1.72" method="GET" user="kubecfg" as="<self>" asgroups="<lookup>" namespace="default" uri="/api/v1/namespaces/default/pods"
# 2017-02-09T00:15:57.993528822Z AUDIT: id="90c73c7c-97d6-4b65-9461-f94606ff825f" response="200"
<source>
  @type tail
  @id in_tail_kube_apiserver_audit
  multiline_flush_interval 5s
  path /var/log/kubernetes/kube-apiserver-audit.log
  pos_file /var/log/kube-apiserver-audit.log.pos
  tag kube-apiserver-audit
  <parse>
    @type multiline
    format_firstline /^\S+\s+AUDIT:/
    # Fields must be explicitly captured by name to be parsed into the record.
    # Fields may not always be present, and order may change, so this just looks
    # for a list of key="\"quoted\" value" pairs separated by spaces.
    # Unknown fields are ignored.
    # Note: We can't separate query/response lines as format1/format2 because
    #       they don't always come one after the other for a given query.
    format1 /^(?<time>\S+) AUDIT:(?: (?:id="(?<id>(?:[^"\\]|\\.)*)"|ip="(?<ip>(?:[^"\\]|\\.)*)"|method="(?<method>(?:[^"\\]|\\.)*)"|user="(?<user>(?:[^"\\]|\\.)*)"|groups="(?<groups>(?:[^"\\]|\\.)*)"|as="(?<as>(?:[^"\\]|\\.)*)"|asgroups="(?<asgroups>(?:[^"\\]|\\.)*)"|namespace="(?<namespace>(?:[^"\\]|\\.)*)"|uri="(?<uri>(?:[^"\\]|\\.)*)"|response="(?<response>(?:[^"\\]|\\.)*)"|\w+="(?:[^"\\]|\\.)*"))*/
    time_format %Y-%m-%dT%T.%L%Z
  </parse>
</source>

<filter kubernetes.**>
  @type kubernetes_metadata
  @id filter_kube_metadata
</filter>

----------------------------
prometheus.conf
--------------------------

# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/prometheus.conf.erb

# Prometheus metric exposed on 0.0.0.0:24231/metrics
<source>
  @type prometheus
  bind "#{ENV['FLUENTD_PROMETHEUS_BIND'] || '0.0.0.0'}"
  port "#{ENV['FLUENTD_PROMETHEUS_PORT'] || '24231'}"
  metrics_path "#{ENV['FLUENTD_PROMETHEUS_PATH'] || '/metrics'}"
</source>

<source>
  @type prometheus_output_monitor
</source>

--------------------------
systemd.conf
-----------------------------------
# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/systemd.conf.erb

# Logs from systemd-journal for interesting services.
<source>
  @type systemd
  @id in_systemd_kubelet
  matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
  <storage>
    @type local
    persistent true
    path /var/log/fluentd-journald-kubelet-cursor.json
  </storage>
  <entry>
    fields_strip_underscores true
  </entry>
  read_from_head true
  tag kubelet
</source>

# Logs from docker-systemd
<source>
  @type systemd
  @id in_systemd_docker
  matches [{ "_SYSTEMD_UNIT": "docker.service" }]
  <storage>
    @type local
    persistent true
    path /var/log/fluentd-journald-docker-cursor.json
  </storage>
  <entry>
    fields_strip_underscores true
  </entry>
  read_from_head true
  tag docker.systemd
</source>

# Logs from systemd-journal for interesting services.
<source>
  @type systemd
  @id in_systemd_bootkube
  matches [{ "_SYSTEMD_UNIT": "bootkube.service" }]
  <storage>
    @type local
    persistent true
    path /var/log/fluentd-journald-bootkube-cursor.json
  </storage>
  <entry>
    fields_strip_underscores true
  </entry>
  read_from_head true
  tag bootkube
</source>
-----------------------------------------------------




<source>
 @type tail
 read_from_head true
 format json
 tag containers-logs
 path /var/log/containers/*.log
 <parse>
  @type json
 </parse>
 pos_file /tmp/contianer-logs.pos
</source>

<match containers-logs>
 @type elasticsearch
 host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
 port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
 scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
 index_name pods_logs
 type_name fluentd
</match>


/var/log/pods/*/*

------------------------------------------------
fluentd image:  fluent/fluentd-kubernetes-daemonset:v1.4.2-debian-elasticsearch-1.1

COPY fluent.conf /fluentd/etc/

ENV FLUENTD_CONF=fluent.conf
ENTRYPOINT ["tini" "--" "/fluentd/entrypoint.sh"]

---------------------------------------

FROM fluent/fluentd:v1.15-debian-1

# Use root account to use apt
USER root

# below RUN includes plugin as examples elasticsearch is not required
# you may customize including plugins as you wish
RUN buildDeps="sudo make gcc g++ libc-dev" \
 && apt-get update \
 && apt-get install -y --no-install-recommends $buildDeps \
 && sudo gem install fluent-plugin-elasticsearch \
 && sudo gem sources --clear-all \
 && SUDO_FORCE_REMOVE=yes \
    apt-get purge -y --auto-remove \
                  -o APT::AutoRemove::RecommendsImportant=false \
                  $buildDeps \
 && rm -rf /var/lib/apt/lists/* \
 && rm -rf /tmp/* /var/tmp/* /usr/lib/ruby/gems/*/cache/*.gem

COPY fluent.conf /fluentd/etc/
COPY entrypoint.sh /bin/

USER fluent

EXPOSE 24224 5140


---------------------------------------

docker run -d -p 24224:24224 -p 5140:5140 -it --rm --name custom-docker-fluent-logger -v $(pwd)/log:/fluentd/log  vinaytalla/fluentd:v1

docker inspect -f '{{.NetworkSettings.IPAddress}}' custom-docker-fluent-logger

docker run --log-driver=fluentd --log-opt tag="docker.{{.ID}}" --log-opt fluentd-address=192.168.2.113:24224 python:alpine echo Hello

--log-driver=fluentd --log-opt tag="docker.{{.ID}}" --log-opt fluentd-address=192.168.2.113:24224 echo Hello

docker kill -s USR1 custom-docker-fluent-logger


replace FLUENTD.ADD.RE.SS with actual IP address you inspected at the previous step)

You will see some logs sent to Fluentd.

--------------------------------------------------------------

  official fluentd configuration 
----------------------------------------------

symlink has beem created for our mountPath  ---->  /var/log/pods
 lrwxrwxrwx 1 root root 80 Aug 29 05:37 curl-pod_default_curl-ae71b9d3a0829e4c28593d1997d9f01630de3525ec84108bf5ca80e240cb8284.log -> /var/log/pods/default_curl-pod_bd6d4699-ead2-4211-8e8c-69cd264607ee/curl/142.log

permissions for the configuration files in /fluentd/etc/

-rw-r--r-- 1 root root 1199 Jul  3  2019 systemd.conf
-rw-r--r-- 1 root root  421 Jul  3  2019 prometheus.conf
-rw-r--r-- 1 root root 4899 Jul  3  2019 kubernetes.conf
-rw-r--r-- 1 root root    0 Jul  3  2019 disable.conf
-rw-r--r-- 1 root root 1854 Aug 29 05:35 fluent.conf


--------------------------
fluent.conf
-------------------------

# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/fluent.conf.erb

@include "#{ENV['FLUENTD_SYSTEMD_CONF'] || 'systemd'}.conf"
@include "#{ENV['FLUENTD_PROMETHEUS_CONF'] || 'prometheus'}.conf"
@include kubernetes.conf
@include conf.d/*.conf

<match **>
   @type elasticsearch
   @id out_es
   @log_level info
   include_tag_key true
   host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
   port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
   path "#{ENV['FLUENT_ELASTICSEARCH_PATH']}"
   scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
   ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
   ssl_version "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERSION'] || 'TLSv1'}"
   reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'false'}"
   reconnect_on_error "#{ENV['FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR'] || 'true'}"
   reload_on_failure "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE'] || 'true'}"
   log_es_400_reason "#{ENV['FLUENT_ELASTICSEARCH_LOG_ES_400_REASON'] || 'false'}"
   logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
   logstash_format "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT'] || 'true'}"
   index_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_INDEX_NAME'] || 'logstash'}"
   type_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_TYPE_NAME'] || 'fluentd'}"
   <buffer>
     flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
     flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
     chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
     queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
     retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
     retry_forever true
   </buffer>
</match>



--------------------------------------------
kubernetes.conf
-------------------

<source>
  @type tail
  @id in_tail_container_logs
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type "#{ENV['FLUENT_CONTAINER_TAIL_PARSER_TYPE'] || 'json'}"
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>


<match containers-logs>
 @type elasticsearch
 host elasticsearch
 port 9200
 index_name pods_logs
 type_name fluentd
</match>

---------------------------------------

<source>
 @type tail
 read_from_head true
 tag containers-logs
 path /var/log/containers/*.log
 <parse>
  @type json
 </parse>
 pos_file /tmp/contianer-logs.pos
</source>

<match containers-logs>
 @type elasticsearch
 host elasticsearch
 port 9200
 index_name pods_logs
 type_name fluentd
</match>
-------------------------------------------
 env:
          - name:  FLUENT_ELASTICSEARCH_HOST
            value: "elasticsearch.efk.svc.cluster.local"
          - name:  FLUENT_ELASTICSEARCH_PORT
            value: "9200"
          - name: FLUENT_ELASTICSEARCH_SCHEME
            value: "http"
----------------------------------------------

logs after using custom fluent.conf file

--------------------------
logs of fluentd pod


exec into fluentd pod

ls -lrt /var/log/containers 

lrwxrwxrwx 1 root root 101 Aug 29 04:17 coredns-64897985d-jlz9l_kube-system_coredns-1871db594f32b968a15e41c7e332672ffe3d8c5d0ad76020ee1ae2218672b9bf.log -> /var/log/pods/kube-system_coredns-64897985d-jlz9l_db040296-fdf3-4310-bddf-93198fc40066/coredns/37.log
lrwxrwxrwx 1 root root  82 Aug 29 05:06 fluentd-q5k5x_efk_fluentd-d50db6fcfaece99c4fcebd7fa7eb8c20879ee5b3e6f428237ffe694988b26d26.log -> /var/log/pods/efk_fluentd-q5k5x_3cbfb9e2-4534-4e96-aef5-bb0271a5f326/fluentd/0.log
---------------------------

2022-08-29 04:51:30 +0000 [info]: gem 'fluentd' version '1.4.2'
2022-08-29 04:51:30 +0000 [info]: adding match pattern="containers-logs" type="elasticsearch"
2022-08-29 04:51:32 +0000 [warn]: #0 Could not communicate to Elasticsearch, resetting connection and trying again. Connection refused - connect(2) for 172.16.95.251:9200 (Errno::ECONNREFUSED)
2022-08-29 04:51:36 +0000 [warn]: #0 Could not communicate to Elasticsearch, resetting connection and trying again. Connection refused - connect(2) for 172.16.95.251:9200 (Errno::ECONNREFUSED)
2022-08-29 04:51:44 +0000 [warn]: #0 Could not communicate to Elasticsearch, resetting connection and trying again. Connection refused - connect(2) for 172.16.95.251:9200 (Errno::ECONNREFUSED)
2022-08-29 04:52:00 +0000 [warn]: #0 Could not communicate to Elasticsearch, resetting connection and trying again. Connection refused - connect(2) for 172.16.95.251:9200 (Errno::ECONNREFUSED)

2022-08-29 04:52:33 +0000 [warn]: #0 Detected ES 7.x or above: `_doc` will be used as the document `_type`.
2022-08-29 04:52:33 +0000 [warn]: #0 To prevent events traffic jam, you should specify 2 or more 'flush_thread_count'.
2022-08-29 04:52:33 +0000 [info]: adding source type="tail"
2022-08-29 04:52:33 +0000 [warn]: parameter 'format' in <source>
  @type tail
  read_from_head true
  format json
  tag "containers-logs"
  path "/var/log/containers/*.log"
  pos_file "/tmp/contianer-logs.pos"
  <parse>
    @type "json"
  </parse>
</source> is not used.
2022-08-29 04:52:33 +0000 [info]: #0 starting fluentd worker pid=13 ppid=8 worker=0
2022-08-29 04:52:33 +0000 [warn]: #0 /var/log/containers/calico-node-k7fpv_kube-system_calico-node-0d0ca5866990548d6c1e0f293955a59cf130b1a22bf3bdc9f0e7f5648004b586.log unreadable. It is excluded and would be examined next time.

---------------------------------
logs of elasticsearch pod
---------------------------------

{"type": "server", "timestamp": "2022-08-26T09:44:35,040+0000", "level": "INFO", "component": "o.e.c.m.MetaDataMappingService", "cluster.name": "k8s-logs", "node.name": "es-cluster-0", "cluster.uuid": "7XSdIj5NStasks-w5Cn3yw", "node.id": "_hJgpEenQZiKa66LZiovVA",  "message": "[.kibana_1/ggngoWPDTliICfnKWV0ung] update_mapping [_doc]"  }
{"type": "deprecation", "timestamp": "2022-08-26T09:44:48,590+0000", "level": "WARN", "component": "o.e.d.s.a.b.h.DateHistogramAggregationBuilder", "cluster.name": "k8s-logs", "node.name": "es-cluster-0", "cluster.uuid": "7XSdIj5NStasks-w5Cn3yw", "node.id": "_hJgpEenQZiKa66LZiovVA",  "message": "[interval] on [date_histogram] is deprecated, use [fixed_interval] or [calendar_interval] in the future."  }
{"type": "deprecation", "timestamp": "2022-08-26T09:45:12,018+0000", "level": "WARN", "component": "o.e.d.i.q.QueryShardContext", "cluster.name": "k8s-logs", "node.name": "es-cluster-0", "cluster.uuid": "7XSdIj5NStasks-w5Cn3yw", "node.id": "_hJgpEenQZiKa66LZiovVA",  "message": "[types removal] Using the _type field in queries and aggregations is deprecated, prefer to use a field instead."  }
{"type": "deprecation", "timestamp": "2022-08-26T09:45:16,333+0000", "level": "WARN", "component": "o.e.d.s.a.b.h.DateHistogramAggregationBuilder", "cluster.name": "k8s-logs", "node.name": "es-cluster-0", "cluster.uuid": "7XSdIj5NStasks-w5Cn3yw", "node.id": "_hJgpEenQZiKa66LZiovVA",  "message": "[interval] on [date_histogram] is deprecated, use [fixed_interval] or [calendar_interval] in the future."  }

----------------------
logs of kibana pod
---------------------

AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36","referer":"http://192.168.2.100:31086/app/kibana","accept-encoding":"gzip, deflate","accept-language":"en-US,en;q=0.9"},"remoteAddress":"172.16.219.64","userAgent":"172.16.219.64","referer":"http://192.168.2.100:31086/app/kibana"},"res":{"statusCode":200,"responseTime":27,"contentLength":9},"message":"GET /api/rollup/indices 200 27ms - 9.0B"}
{"type":"response","@timestamp":"2022-08-26T09:56:34Z","tags":[],"pid":1,"method":"post","statusCode":200,"req":{"url":"/elasticsearch/*/_search?ignore_unavailable=true","method":"post","headers":{"host":"192.168.2.100:31086","connection":"keep-alive","content-length":"69","accept":"application/json, text/plain, */*","kbn-version":"7.2.0","user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36","content-type":"application/json","origin":"http://192.168.2.100:31086","referer":"http://192.168.2.100:31086/app/kibana","accept-encoding":"gzip, deflate","accept-language":"en-US,en;q=0.9"},"remoteAddress":"172.16.219.64","userAgent":"172.16.219.64","referer":"http://192.168.2.100:31086/app/kibana"},"res":{"statusCode":200,"responseTime":17,"contentLength":9},"message":"POST /elasticsearch/*/_search?ignore_unavailable=true 200 17ms - 9.0B"}



-------------------------------------------
ls -lZ /var/lib/docker/containers/

find . -type f -iname *.log

 curl http://elasticsearch.efk:9200/_cat/indices   --->  to test index patterns 




entrypoint.sh

root@fluentd-rdvhf:/fluentd# cat entrypoint.sh
#!/bin/sh


set -e

if [ -z ${FLUENT_ELASTICSEARCH_SED_DISABLE} ] ; then
  if [ -z ${FLUENT_ELASTICSEARCH_USER} ] ; then
    sed -i  '/FLUENT_ELASTICSEARCH_USER/d' /fluentd/etc/${FLUENTD_CONF}
  fi

  if [ -z ${FLUENT_ELASTICSEARCH_PASSWORD} ] ; then
    sed -i  '/FLUENT_ELASTICSEARCH_PASSWORD/d' /fluentd/etc/${FLUENTD_CONF}
  fi
fi

exec fluentd -c /fluentd/etc/${FLUENTD_CONF} -p /fluentd/plugins --gemfile /fluentd/Gemfile ${FLUENTD_OPT}



--------------------------
flunet-bit
----------------------------

---------------
configMap
--------------
apiVersion: v1
data:
  parsers.conf: |-
    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
  custom_parsers.conf: |
    [PARSER]
        Name docker_no_time
        Format json
        Time_Keep Off
        Time_Key time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
  fluent-bit.conf: |
    [SERVICE]
        Flush 1
        Daemon Off
        Log_Level info
        Parsers_File parsers.conf
        HTTP_Server On
        HTTP_Listen 0.0.0.0
        HTTP_Port 2020
    [INPUT]
        Name tail
        Path /var/log/containers/*.log
        Parser docker
        Tag kube.*
        Mem_Buf_Limit 5MB
        Skip_Long_Lines On
    [FILTER]
        Name kubernetes
        Match kube.*
        Merge_Log On
        Merge_Log_Trim On
        Labels On
        Annotations Off
        K8S-Logging.Parser Off
        K8S-Logging.Exclude Off
    [INPUT]
        Name systemd
        Tag host.*
        Systemd_Filter _SYSTEMD_UNIT=kubelet.service
        Read_From_Tail On
    [OUTPUT]
        Name es
        Match kube.*
        Host ${FLUENT_ELASTICSEARCH_HOST}
        Port  ${FLUENT_ELASTICSEARCH_PORT}
        Index my_index
kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: fluent-bit-config
    meta.helm.sh/release-namespace: default
  labels:
    app.kubernetes.io/instance: fluent-bit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: fluent-bit
    app.kubernetes.io/version: 1.7.9
    helm.sh/chart: fluent-bit-0.15.15
  name: fluent-bit
  namespace: logging

-----------------------
DaemonSet fluent-bit
-------------------------

apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: efk
  labels:
    app: fluentd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit
  labels:
    app: fluentd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluent-bit
roleRef:
  kind: ClusterRole
  name: fluent-bit
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: efk
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: logging
  labels:
    k8s-app: fluent-bit-logging
    version: v1
    kubernetes.io/cluster-service: "true"
spec:
  selector:
    matchLabels:
      k8s-app: fluent-bit-logging
  template:
    metadata:
      labels:
        k8s-app: fluent-bit-logging
        version: v1
        kubernetes.io/cluster-service: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "2020"
        prometheus.io/path: /api/v1/metrics/prometheus
    spec:
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:1.5
        imagePullPolicy: Always
        ports:
          - containerPort: 2020
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch-master.default"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc/
      terminationGracePeriodSeconds: 10
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config
      serviceAccountName: fluent-bit
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - operator: "Exists"
        effect: "NoExecute"
      - operator: "Exists"
        effect: "NoSchedule"